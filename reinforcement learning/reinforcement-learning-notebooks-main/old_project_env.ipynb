{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rl_project_env.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["World Definition"],"metadata":{"id":"ZilHSEW7KcAU"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.pylab import rcParams\n","plt.style.use('ggplot')\n","rcParams['figure.figsize'] = 10, 10\n","\n","dimension = 10\n","\n","# represents the size of the world\n","main_axis = ['', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n","grid_axis = np.linspace(0, 13, 13)\n","grid_interval = grid_axis[1] - grid_axis[0]\n","door_width = 0.1\n","grid_world = np.zeros((dimension+1, dimension+1))\n","\n","actions = {0:'pick up', 1:'use', 5:'drop', 8:'up', 4: 'left', 2: 'down', 6:'right', 3:\"do nothing\"}\n","\n","class EscapeRoom:\n","\n","  def __init__(self):\n","    \"\"\"Initializes everything\"\"\"\n","\n","    \"\"\"Default Values\"\"\"\n","    DEFAULT_PARAMS = {'p1' : [4, 6], 'p2' : [2, 1], 'k1' : [2, 4], 'k2' : [3, 9],\n","                  'd1' : [0, 5], 'd2' : [5,11], 'i1' : [], 'i2' : []}\n","    \n","    # defining keys and doors\n","    self.d1 = DEFAULT_PARAMS['d1']\n","    self.d2 = DEFAULT_PARAMS['d2']\n","    self.k1 = DEFAULT_PARAMS['k1']\n","    self.k2 = DEFAULT_PARAMS['k2']\n","\n","    # agents\n","    self.p1 = DEFAULT_PARAMS['p1']\n","    self.p2 = DEFAULT_PARAMS['p2']\n","\n","    # inventory\n","    self.i1 = DEFAULT_PARAMS['i2']\n","    self.i2 = DEFAULT_PARAMS['i1']\n","\n","  def reset(self):\n","\n","    \"\"\"Reset the board from start\"\"\"\n","    self.__init__()\n","    return None\n","  \n","  def get_reward(self):\n","    \"\"\"returns reward for every action\"\"\"\n","    reward_array = [-1, -1]\n","\n","    if self.p1 == self.d1:\n","      reward_array[0] = 10\n","\n","    if self.p2 == self.d2:\n","      reward_array[1] = 10\n","\n","    return reward_array\n","\n","  def execute_1(self, action = 3):\n","    \"\"\"Executes action for agent p1\"\"\"\n","\n","    # movements\n","    if action == 8:\n","      self.p1[1] += 1\n","    elif action == 4:\n","      self.p1[0] -= 1\n","    elif action == 2:\n","      self.p1[1] -= 1\n","    elif action == 6:\n","      self.p1[0] += 1\n","    \n","    # picking up keys\n","    elif action == 0 and (self.p1 == self.k1 or self.p1 == self.k2):\n","      if self.p1 == self.k1:\n","        self.i1.append(\"k1\")\n","        self.k1 = None\n","      else:\n","        self.i1.append(\"k2\")\n","        self.k2 = None\n","    \n","    # using keys\n","    elif action == 1 and np.sum(np.abs(np.asarray(self.p1) - np.asarray(self.d1))) == 1.0 and \"k1\" in self.i1:\n","        self.p1 = self.d1\n","        self.k1 = None\n","    # dropping keys\n","    elif action == 5 and len(self.i1) > 0:\n","        k = self.i1.pop()\n","        if k == \"k1\":\n","          self.k1 = self.p1\n","        else:\n","          self.k2 = self.p1\n","    else:\n","      pass\n","\n","\n","    # dont crash to the wall\n","    if self.p1[0] in [0,11] or self.p1[1] in [0,11]:\n","        self.__init__()\n","        return -1\n","    \n","    return 1\n","\n","  def execute_2(self, action = 3):\n","    \"\"\"Executes action for agent p2\"\"\"\n","\n","    if action == 8:\n","        self.p2[1] += 1\n","    elif action == 4:\n","      self.p2[0] -= 1\n","    elif action == 2:\n","      self.p2[1] -= 1\n","    elif action == 6:\n","      self.p2[0] += 1\n","    elif action == 3:\n","      pass\n","    else:\n","      pass\n","\n","    # dont crash to the wall\n","    if self.p2[0] in [0,11] or self.p2[1] in [0,11]:\n","        self.__init__()\n","        return -1\n","\n","  def show(self):\n","    \n","    # to change positions\n","    agent1 = plt.Circle((grid_interval*(self.p1[0]+0.5),grid_interval*(self.p1[1]+0.5)), 0.3, color='r', alpha = 0.5)\n","    agent2 = plt.Circle((grid_interval*(self.p2[0]+0.5),grid_interval*(self.p2[1]+0.5)), 0.3, color='blue', alpha = 0.5)\n","    \n","    door1 = plt.Rectangle((self.d1[0]*grid_interval, self.d1[1]*grid_interval), grid_interval, grid_interval, color = \"red\", alpha = 0.8)\n","    door2 = plt.Rectangle((self.d2[0]*grid_interval, self.d2[1]*grid_interval), grid_interval, grid_interval, color = \"blue\", alpha = 0.8)\n","    \n","    if self.k1:\n","      key1 = plt.Rectangle(((self.k1[0]+0.375)*grid_interval,(self.k1[1]+0.25)*grid_interval), 0.25, 0.5, angle = -10, color = \"red\", alpha = 0.5)\n","    if self.k2:\n","      key2 = plt.Rectangle(((self.k2[0]+0.375)*grid_interval,(self.k2[1]+0.25)*grid_interval), 0.25, 0.5, angle = -10, color = \"blue\", alpha = 0.5)\n","\n","    # wall -> DO NOT CHANGE\n","    wall1 = plt.Rectangle((0,0), grid_interval, 13, color = \"black\")\n","    wall2 = plt.Rectangle((0,0), 13, grid_interval, color = \"black\")\n","    wall4 = plt.Rectangle((0,grid_axis[-2]), 13, grid_axis[1], color = \"black\")\n","    wall3 = plt.Rectangle((grid_axis[-2],0),grid_axis[1], 13, color = \"black\")\n","\n","    # get plot axis\n","    fig = plt.figure(figsize = (10,10))\n","    ax = plt.gca()\n","\n","    # wall -> DO NOT CHANGE\n","    ax.add_patch(wall1)\n","    ax.add_patch(wall2)\n","    ax.add_patch(wall3)\n","    ax.add_patch(wall4)\n","\n","    # world definintions \n","    ax.add_patch(door1)\n","    ax.add_patch(door2)\n","    if self.k1:\n","      ax.add_patch(key1)\n","    if self.k2:\n","      ax.add_patch(key2)\n","\n","    # plot agents\n","    ax.add_patch(agent1)\n","    ax.add_patch(agent2)\n","\n","    plt.xticks(ticks = grid_axis, labels = main_axis)\n","    plt.yticks(ticks = grid_axis, labels = main_axis)\n","    plt.show()"],"metadata":{"id":"0DQTCuu6KdE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# given state and action, return next state and reward\n","\n","current_state = np.asarray([1, 2])"],"metadata":{"id":"LfLLIVvOu9E7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(10):\n","  world.execute_1(action = 4)\n","world.render()"],"metadata":{"id":"ItVYCpW6Rz8S","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"error","timestamp":1649780510438,"user_tz":-330,"elapsed":17,"user":{"displayName":"Surya Prasath R","userId":"18215364500953637153"}},"outputId":"2e2f01c5-59a9-4a3b-92df-381c324d1a87"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-f66bf9174e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'EscapeRoom' object has no attribute 're'"]}]},{"cell_type":"markdown","source":["## CASES TO CONSIDER\n","\n","\n","1.   **Impact of considering others**: Go to blue door, open it, take something and should close it and then go to red. Same objective for other agent, who does it after some time. (considering future tasks)\n","2.    **Illustration of optimal behaviours**: The agent uses the phone and must leave the phone close to the blue door (where the other agent resides, before leaving)\n","3.    **Varying the caring coeffecient**: How far does the agent go, when he wants to take other agent's key and leave it close to him. (show that too much caring can infact decrease the reward)\n","\n","\n"],"metadata":{"id":"CqmczPsbH4pR"}}]}